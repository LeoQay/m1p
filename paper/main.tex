\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}



\title{Метод построения случайного леса на основе отдаления друг от друга базовых моделей}

\author{ Дмитриев Леонид Алексеевич  \\
	МГУ им. М.В. Ломоносова\\
	ф-т ВМК, кафедра ММП\\
	% Pittsburgh, PA 15213 \\
	\texttt{s02200542@gse.cs.msu.ru} \\
	%% examples of more authors
	\And
	д.ф-м.н. Сенько Олег Валентинович \\
	МГУ им. М.В. Ломоносова\\
	ф-т ВМК, кафедра ММП\\
	% Santa Narimana, Levand \\
	\texttt{senkoov@mail.ru } \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{Метод случайного леса}


\begin{document}
\maketitle

\begin{abstract}
	В работе представлен новый метод случайного леса, который строится итеративно и в котором новое дерево обучается с учетом накопленного до него ансамбля. Данный метод хорошо проявляет себя на некоторых химических и медицинских данных. Исследование метода проводилось на датасете кристаллических решеток.
\end{abstract}


\keywords{Случайный лес \and Решающие деревья}

\section{Introduction}

Случайный лес - широко известный алгоритм машинного обучения.
Его способность хорошо аппроксимировать данные за счёт
уменьшения разброса основана на предположении, что все деревья
ансамбля независимы и различны. Но практическое уменьшение
разброса значительно меньше теоретического, так как на деле
получаемые деревья обучаются на объектах из одного и того же
множества. Это проблема частично решается такими методами, как
беггинг и бутстрап.
Предлагается другой способ повышения разнообразия деревьев
внутри леса. Вместо независимой и параллельной генерации деревьев,
будем на каждом шагу добавлять дерево, сильно отличающееся от уже
созданного ансамбля, с помощью специального функционала,
учитывающего ответы предыдущих моделей. Выдвигается гипотеза,
что данный метод повысит разнообразие деревьев в ансамбле и тем
самым уменьшит разброс предсказаний.
Помимо отличия от предыдущих моделей, следующему дереву можно
придать свойства какого-либо другого метода, например градиентного
бустинга. Это может сделать лес не только разнообразным, но и более
качественным.
В данной работе будет детально рассмотрено построение дерева на
очередном шаге по вышеописанному методу и для тестирования будет
исследовано его применение на данных о химических веществах.

\section{Problem statement}
\label{sec:headings}

\subsection{Теория}

Предположим, что у нас есть переменная $Y$, стохастически зависящая от вектора переменных $X_1, \dots, X_n$, 
функции $G_1(x)$, $G_2(x)$, детерминировано зависящие от вектора переменных $X_1, \dots, X_n$.

Имеется выборка $\tilde{S} = \{ (y_j, x_j, G_1(x_j), G_2(x_j)), j = \overline{1, m} \}$, где
\begin{itemize}
	\item $y_j$ - значение переменной $Y$ для объекта с номером $j$
	\item $x_j = (x_{j1}, \dots, x_{jn})$ - вектор значений признаков $X_1, \dots, X_n$ для объекта с номером $j$
	\item $G_1(x_j)$ - значение функции $G_1$ в точке $x_j$
	\item $G_2(x_j)$ - значение функции $G_2$ в точке $x_j$
\end{itemize}

Предлагается построить дерево $T(x)$, для которого достигается минимум функционала:
$$ \Phi(\tilde{S}, T) = \Sigma_{j=1}^{m} \{ \gamma_1 [T(x_j) - y_j]^2 + \gamma_2 [T(x_j) - G_2(x_j)]^2 - \mu [T(x_j) - G_1(x_j)]^2   \}  $$
где $\gamma_1 + \gamma_2 = 1; \quad \gamma_1, \gamma_2, \mu \in [0, 1] $


Как видно из структуры функционала $\Phi$, дерево $T$, соответствующее его минимуму,  будет аппроксимировать связь $Y$ с переменными $X_1, \dots, X_n$ при $\gamma_1 > 0$.

Одновременно дерево $T(x)$ будет удаляться от зависимости $G_2(x)$ при возрастании $\mu$ и приближаться к зависимости $G_1(x)$ при возрастании $\gamma_2$.


\subsection{Реализация дерева}

При построении дерева был использован "жадный" \space метод оптимизации целевого функционала: на каждом шагу к дереву добавляется узел, обеспечивающий наибольшее снижение используемого функционала $\Phi$.

Предположим, что на каком-то шаге дерево $T_k$ содержит $k$ концевых узлов, которым соответствуют концевые выборки $S_1^k, \dots, S_k^k$.

Новое дерево $T_{k + 1}$ строится через добавление к дереву $T_k$ дополнительного узла $u$.

Узел $u$ получается из некоторого концевого узла $g$ с помощью порогового правила вида $X_u > \delta_u$, где $X_u$ и $\delta_u$ признак и порог к нему соответственно. 

Правило $X_u > \delta_u$ расщепляет выборку $S_g^k$ на две подвыборки.

Признак $X_u$ и порог $\delta_u$ ищутся из условия максимизации разности $\Phi(\tilde{S}, T_k) - \Phi(\tilde{S}, T_{k + 1})$.

Процесс построения может быть прекращен при выполнении одного из перечисленных условий:

\begin{itemize}
	\item На очередном шаге не удается уменьшить функционал
	\item На очередном шаге произошло изменение функционала, меньшее чем некоторое пороговое значение
	\item Кол-во объектов внутри узла меньше некоторого порогового значения
\end{itemize}


\vspace{1.3cm}

\section{Headings: first level}
\label{sec:headings}

\lipsum[4] See Section \ref{sec:headings}.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
	\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}
\lipsum[7]



\section{Examples of citations, figures, tables, references}
\label{sec:others}

\subsection{Citations}
Citations use \verb+natbib+. The documentation may be found at
\begin{center}
	\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}

Here is an example usage of the two main commands (\verb+citet+ and \verb+citep+): Some people thought a thing \citep{kour2014real, hadash2018estimate} but other people thought something else \citep{kour2014fast}. Many people have speculated that if we knew exactly why \citet{kour2014fast} thought this\dots

\subsection{Figures}
\lipsum[10]
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11]



\subsection{Tables}
See awesome Table~\ref{tab:table}.

The documentation for \verb+booktabs+ (`Publication quality tables in LaTeX') is available from:
\begin{center}
	\url{https://www.ctan.org/pkg/booktabs}
\end{center}


\begin{table}
	\caption{Sample table title}
	\centering
	\begin{tabular}{lll}
		\toprule
		\multicolumn{2}{c}{Part}                   \\
		\cmidrule(r){1-2}
		Name     & Description     & Size ($\mu$m) \\
		\midrule
		Dendrite & Input terminal  & $\sim$100     \\
		Axon     & Output terminal & $\sim$10      \\
		Soma     & Cell body       & up to $10^6$  \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
	\item Lorem ipsum dolor sit amet
	\item consectetur adipiscing elit.
	\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\newpage


\addcontentsline{toc}{section}{Список используемой литературы}

%далее сам список используевой литературы
\begin{thebibliography}{}
	\bibitem{litlink1} O.V. Senko, A.A. Dokukin, N.N. Kiselyova, V.A. Dudarev
	, Yu.O. Kuznetsova - "New Two-Level Ensemble Method and Its Application to
	Chemical Compounds Properties Prediction"
	\bibitem{litlink2} Liu, Y., Wang, Y., Zhang, J. (2012). New Machine Learning Algorithm: Random Forest. In: Liu, B., Ma, M., Chang, J. (eds) Information Computing and Applications. ICICA 2012. Lecture Notes in Computer Science, vol 7473. Springer, Berlin, Heidelberg.
\end{thebibliography}

\end{document}
